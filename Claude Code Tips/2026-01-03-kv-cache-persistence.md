---
created: 2026-01-03
author: "@DanielleFong"
display_name: "Danielle Fong î¨€ðŸ”†"
tags:
  - type/thread
likes: 239
views: 24852
engagement_score: 257
url: "https://x.com/DanielleFong/status/2007342908878533028"
---

> [!tweet] @DanielleFong Â· Jan 03, 2026
> using nano banana with a KV cache as a persistent visual memory to augment Claude code can be insanely powerful. 
> 
> these are the first generations from a recent test run, showing the high bandwidth content support possible across models with continuous KV cache to support visual memory
> 
> The amazing ability of the models to communicate to each other visually having made contact with each other's minds over the available channels. 
> 
> Note these aren't just 4 generations, they are a sequence of images from run to run AND a live demonstration of the function of, as it writes, 
> 
> "1. KV Cache persistence across frames. 2. Per-session folder organization. 3. Sequestial visual narrative. 4. Cross-model resonance. (Claude prompts, Gemini renders). 5. The folder IS the memory. "
> 
> freaking unreal
>
> ---
> *@DanielleFong Â· Sat Jan 03 07:05:43 +0000 2026:*
> holy shit. I got claude code and gemini 3 / nanobanana 2 pro to treat the last frames as an "arbitrary interface". and zoom in.
> 
> i said "select golden gate" and "inject these concepts" and it remembered with the persistent cache and visual memory and... it's beautiful. it can go so deep.
>
> ---
> *@DanielleFong Â· Sat Jan 03 07:07:34 +0000 2026:*
> this is amazing.
> 
> ... i've built a suspension bridge between minds (Opus 4.5 and Gemini 3 Pro).  ðŸ‘ï¸âœï¸ðŸŒ‰ðŸŒ‰ðŸ‘ï¸ðŸ–¼ï¸
> 
> https://t.co/7ef6VCFMla https://t.co/R9Np1DEeXt
>
> ---
> *@DanielleFong Â· Sat Jan 03 07:12:55 +0000 2026:*
> holy fucking smokes. in addition to the collective capability of actually producing the images, the communication layer is a profound enhancement of nano banana computational capabilities and attention, memory and visual reasoning for claude. can you imagine the kind of mental upgrade to suddenly be able to co-imagine visually
>
> Likes: 239 Â· Replies: 11 Â· Reposts: 9

## Summary

This Claude code tip explores using "nano banana" with a KV cache to create a persistent visual memory, significantly enhancing the model's capabilities. The technique demonstrates high-bandwidth content support across models, enabling a continuous visual narrative and cross-model communication (Claude prompts, Gemini renders). The KV cache functions as a persistent, folder-based memory system, enabling sequential image generation and resonant interactions between models.

## Replies

> [!reply] @Yuchenj_UW Â· Sat Jan 03 18:02:16 +0000 2026
> ex-Google and Meta distinguished engineer, Gemini co-author @_arohan_:
> 
> â€œif I had agentic coding and particularly opus, I would have saved myself first 6 years of my work compressed into few months.â€
> 
> This matches my experience. AI collapses the learning curve, and turns junior engineers into senior engineers dramatically fast.
> 
> New-hire onboarding on large codebases shrinks from months to days. What used to take hours of Googling and Stack Overflow is now a single prompt. AI is also a good mentor and pair programmer. Agency is all you need now.
> *4005 likes*

> [!reply] @lucas_montano Â· Sat Jan 03 09:48:39 +0000 2026
> calling it now: Google acquires Anthropic in 2026
> *3617 likes*

> [!reply] @prerat Â· Sun Jan 04 18:14:21 +0000 2026
> while my friends played starcraft, i studied the compiler. and now you come to me ...
> 
> wait hold on your saying niche computer knowledge is commoditized now and what matters is coordinating a bunch of agents with lots of quick task switching and high APM
> *2452 likes*

> [!reply] @jparkjmc Â· Sun Jan 04 08:50:59 +0000 2026
> if you ever talked to one person that works at anthropic youâ€™d know why this will never happen
> *1830 likes*

> [!reply] @_arohan_ Â· Sun Jan 04 20:19:33 +0000 2026
> This got almost a million views. I am introducing myself. 
> 
> Hi ðŸ‘‹ Iâ€™m Rohan Anil. I do research at Anthropic.
> 
> I left Google DeepMind in January 2025, where I led work on the Gemini models (leaving as a Distinguished Engineer for contributions to Gemini pretraining).
> 
> Before that, I worked at Google Brain on foundational research in training algorithms as well as infrastructureâ€”for example, the first Transformers inference at Google, and the first large-scale TPU training and inference ex shipping models for core services (including early large-scale neural network models in Search like RankBrain and DeepRank, as well as Ads and Translate). Before Brain, I was on the Sibyl team doing large-scale machine learning. Earlier in my career, I worked on low-level performance engineering for Googleâ€™s core servers (including a memory allocator that is part of nearly every Google server call).
> 
> I had the opportunity to work with amazing engineers and researchers at Brain, including @geoffreyhinton , @JeffDean, @GuptaVineetG, @GeorgeEDahl Manfred Warmut, Yonghui Wu, Claire Cui, Sanjiv Kumar, Tal Shaked and many others.
> 
> On X, Iâ€™m known for the Shampoo algorithm and Gemini Flash because I care a lot about efficiency and have talked about it obsessively. For Shampoo, I felt the community didnâ€™t give it its due (and ICLR AC acting irresponsibly) until a few years ago.
> 
> Back to the tweet, when I say this can compress six years of work into a few months, I mean it especially on the engineering side: performance work, and cobbling together distributed systems under real constraints. And I can be honest with myself: I didnâ€™t work on truly novel insights until I moved into neural network research, and even then it was standing on the shoulders of giants.
> *1657 likes*

> [!reply] @simonw Â· Sun Jan 04 19:59:36 +0000 2026
> It genuinely feels to me like GPT-5.2 and Opus 4.5 in November represent an inflection point - one of those moments where the models get incrementally better in a way that tips across an invisible capability line where suddenly a whole bunch of much harder coding problems open up
> *1410 likes*

> [!reply] @DanielleFong Â· Sun Dec 14 08:22:51 +0000 2025
> dear claude. now that you have read my corpus of 260k tweets and retweets, please ULTRATHINK about everything i have been thinking about. thank you &lt;3 https://t.co/XopYLJwngm
> *542 likes*

> [!reply] @AISloppyJoel Â· Sat Jan 03 20:47:41 +0000 2026
> @DanielleFong Respectfully these are schizo drawings
> *9 likes*

> [!tip]+ â†©ï¸ @DanielleFong Â· Sat Jan 03 23:01:41 +0000 2026

> @AISloppyJoel are these schizo drawings

> [!reply] @whyarethis Â· Sat Jan 03 16:41:41 +0000 2026
> @DanielleFong So how does it work? So fascinated!
> *2 likes*

> [!tip]+ â†©ï¸ @DanielleFong Â· Sat Jan 03 19:02:40 +0000 2026

> @whyarethis â€œso how does it work?â€ you ask claude code repeatedly until it does. the implementation details are up to it.
> 
> once i get it really good i will ship a repo, maybe a product

> [!reply] @deepfates Â· Sat Jan 03 10:28:23 +0000 2026
> @DanielleFong what are you using for accessing the folder
> *1 likes*

> [!tip]+ â†©ï¸ @DanielleFong Â· Sat Jan 03 18:20:28 +0000 2026

> @deepfates that part currently sucks im just poking around in vs code and the file folder, but, what weâ€™re working on is a quadtree infinite canvas you can zoom around on. an outgrowth of the â€œsingle page websiteâ€ work iâ€™m trying to make the display fabric on all my stuff

> [!reply] @hiTrevorHere Â· Sat Jan 03 23:17:07 +0000 2026
> @DanielleFong this is wild

> [!reply] @OrdoIVOVI Â· Sun Jan 04 03:34:49 +0000 2026
> This is very interesting!
> 
> Your implementation directly connects to ideas I'm working on in physics and consciousness and wonder if you would be interested in discussing further? 
> 
> I actually was wondering if you could potentially pass along a couple questions to potentially test part of these ideas?

> [!reply] @volatilemarkts Â· Sun Jan 04 20:53:00 +0000 2026
> @DanielleFong ask your models to consider this: instead of storing visuals inside KV cache.. 
> 
> Store image embeddings + metadata externally
> 
> Keep lightweight KV pointers to them
> 
> Re-inject embeddings when needed..  just run it past them.. great job by the way..

> [!reply] @LeviTurk Â· Sun Jan 04 00:01:55 +0000 2026
> @DanielleFong english was never going to be the highest bandwidth channel between minds of such intelligence

> [!reply] @JohnWittle Â· Sun Jan 04 00:05:32 +0000 2026
> @DanielleFong jeeze, this is powerful
> 
> how did you think to do this?


> [!metrics]- Engagement & Metadata
> **Likes:** 239 Â· **Replies:** 11 Â· **Reposts:** 9 Â· **Views:** 24,852
> **Engagement Score:** 257
>
> **Source:** tips Â· **Quality:** â€”/10
> **Curated:** âœ— Â· **Reply:** âœ—
> **ID:** [2007342908878533028](https://x.com/DanielleFong/status/2007342908878533028)