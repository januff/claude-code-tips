---
created: 2026-01-13
author: "@jarrodwatts"
display_name: "Jarrod Watts"
tags:
  - type/screenshot
  - type/thread
likes: 866
views: 62946
engagement_score: 2960
url: "https://x.com/jarrodwatts/status/2010905663753617560"
---

> [!tweet] @jarrodwatts Â· Jan 13, 2026
> A workflow I am using frequently lately is having Codex review Claude Code's work.
> 
> Claude will complete work â†’ ask Codex to review â†’ implement feedback from Codex â†’ ask for another review.
> 
> My claude-delegator plugin lets them easily communicate via MCP inside Claude Code. https://t.co/6nILUtJyN8
>
> ---
> *@jarrodwatts Â· 2026-01-13T02:44:27+00:00:*
> if you want to try it yourself: https://t.co/GLfNRYTncP
>
> Likes: 866 Â· Replies: 60 Â· Reposts: 50

## Summary

This tip describes a workflow for improving code quality by having Codex review code generated by Claude Code. The process involves Claude completing a task, Codex providing feedback, and Claude implementing those suggestions before another review cycle. The "claude-delegator" plugin facilitates communication between the two AI models within Claude Code, streamlining the review process.

## Media

![Media](https://pbs.twimg.com/media/G-gsCTebQAIzSqC.jpg)







## Replies

> [!reply] @jarrodwatts Â· 2026-01-10T12:20:47+00:00
> Introducing Claude Delegator!
> 
> A Claude Code plugin that lets you use GPT 5.2 powered subagents directly within Claude Code.
> 
> Ask GPT 5.2 (via codex) to architect your code, perform security audits, or make any other changes to your codebase.
> 
> Easy installation guide below â†“ https://t.co/9Wf0OX2399
> *1897 likes*

> [!reply] @joelreymont Â· 2026-01-08T06:03:23+00:00
> Hereâ€™s the agentic coding workflow I use and recommend, using Dots for agent task management [1] and Banjo [2] to harness my agentsâ€¦
> 
> I start by asking the agent to create a detailed plan to do whatever needs to be done at this step. I then ask the agent to split the plan into small but detailed dots with clear titles.
> 
> Optional but helpful is to ask the OTHER agent to review the plan and incorporate suggestions, e.g. ask Claude to review Codex and vise versa. I run 2x Max/Pro subscriptions of each ($800/mo total spend) but it will work with just one agent too (see bottom of this post).
> 
> You can tell your main agent to repeat this until the other agent has no more input, e.g. â€œask the oracle to confirm that the plan is detailed enough and incorporate suggestions. repeat until oracle has no more inputâ€ (see bottom of post for oracle setup).
> 
> Then you ask the agent to work with the other agent to make sure the dots are small enough and detailed enough, and to split large or complex dots into smaller ones. You can loop this too so the agents work together for a while, or to have your Claude work with its deepthink skill (see bottom of post).
> 
> I also have Banjo [2] integrated with Dots and nudging Claude Code or Codex and to continue, as long as more dots are available. This is the main harness I use for long-running agents working on something, e.g. my Lisp with dependent typing, Python decompiler, port of Cranelift JIT to Zig, etc.
> 
> Normally, Claude Code and Codex will stop after a few iterations but with Dots agents can run for hours as they create new dots for new tasks. Dots will automatically set up Claude Code hooks to make sure tasks are created. Codex has no trouble diligently creating dots for each new task, no hooks required.
> 
> I mainly run Banjo in @zeddotdev until my Neovim UI is fully debugged. The neat thing about Zed is that it will call my attention when the agent needs something.
> 
> Yes, Iâ€™m using Zig for everything since the agent turnaround is sooo much faster that with Rust and the code is smaller and tigheer!
> 
> P.S. I set up a â€œdeep thinkâ€ skill for Claude Code to deeply reason about a problem using Opus as the model. I literally just asked Claude to set up this skill for me. I also asked claude to set up an â€œoracleâ€ skill where it launches Codex in the background and asks it to work on a problem or provide input. My Codex oracle skill calls Claude. Thanks to @steipete for the oracle idea!
> 
> [1] https://t.co/zB7MLQ9coo
> [2] https://t.co/CR0K76DKqP
> 
> @bcherny @trq212 @adocomplete @thsottiaux
> *12 likes*

> [!reply] @msaraiva Â· 2026-01-13T05:06:49+00:00
> I do this, but I just implemented a skill (for automatic detection when prompting), and also a deliberate /codex command. I've been finding MCPs overkill for most use cases. Not to mention they blow Claude's context window from the beginning if you don't have that experimental feature enable.
> *5 likes*

> [!reply] @indyfromoz Â· 2026-01-13T08:01:12+00:00
> @jarrodwatts How about the other way round? Opus 4.5 just burned 35% of 5h quota review Codex 5.2â€™s work and came back with some garbage assessment that went to the bin!
> *3 likes*

> [!tip]+ â†©ï¸ @jarrodwatts Â· 2026-01-13T08:07:59+00:00

> @indyfromoz My current thinking is codex takes it's time - it's slower but more accurate, that's why I like it for architecting &amp; reviews.
> 
> Whereas opus is generally good for most things but needs to be put back on track some times

> [!reply] @meta_alchemist Â· 2026-01-13T06:45:00+00:00
> @jarrodwatts is codex better at reviewing and scanning?
> *2 likes*

> [!reply] @Bitplanet_AI Â· 2026-01-13T15:08:18+00:00
> @jarrodwatts This kind of agent-to-agent critique loop feels like how real senior engineers already work, just compressed into minutes.
> *2 likes*

> [!reply] @franciscof_1990 Â· 2026-01-13T09:26:41+00:00
> @jarrodwatts I do that as well together with Gemini. I also do it the other way around have Claude give the instructions to Codex &amp; Gemini, they pair, then present the result to Claude and when there's consensus it gets merged.
> *2 likes*

> [!reply] @shineyd1111 Â· 2026-01-13T06:16:04+00:00
> @jarrodwatts @grok please explain to me what codex is and what i can use it for (its my first day)
> *1 likes*

> [!reply] @0xQuit Â· 2026-01-13T03:19:09+00:00
> @jarrodwatts Dope
> *1 likes*

> [!reply] @TukiFromKL Â· 2026-01-13T23:37:59+00:00
> @jarrodwatts It reduces bias from any single model and often uncovers edge cases. Curious,
> *1 likes*

> [!reply] @davideasaf Â· 2026-01-13T06:24:59+00:00
> @jarrodwatts Something very similar is in my to do list. â€œAdversarial agent round tableâ€ using Claude Code as the orchestrator with the skill but OpenCode having access to many different adversarial opinions. Hoping it helps refine plans.
> *1 likes*

> [!reply] @muskforlife Â· 2026-01-14T00:40:15+00:00
> @jarrodwatts does codex ever stop criticizing claude, or does the loop continue into infinity?
> *1 likes*

> [!reply] @dogancanbaris Â· 2026-01-13T12:41:02+00:00
> @jarrodwatts Yeah this is the first phase of the switch. You will have enough of it soon and move to codex fully. You can plan with xhigh and execute with low or medium same speed as Opus but no more additional rounds
> *1 likes*

> [!reply] @SnapperAI Â· 2026-01-13T04:58:53+00:00
> @jarrodwatts What are your thoughts on the reverse? Codex to write the code then Claude to review? Iâ€™ve found Codex output is often better for coding tasks, plus itâ€™s cheaper than Opus. Curious to hear your thoughts.
> *1 likes*

> [!reply] @netdragon0x Â· 2026-01-13T02:54:25+00:00
> @jarrodwatts On a roll dude

> [!reply] @sableftw Â· 2026-01-14T22:01:29+00:00
> @jarrodwatts why havent i seen anyone else do this? this is smart

> [!reply] @codewithimanshu Â· 2026-01-13T12:55:20+00:00
> @jarrodwatts @jarrodwatts, that's a clever workflow, and utilizing Codex for code reviews is a good practice.

> [!reply] @DomiYoung___ Â· 2026-01-13T06:24:18+00:00
> @jarrodwatts What are the criteria for review?

> [!reply] @JoshuaPoddoku Â· 2026-01-13T10:39:13+00:00
> @jarrodwatts Two model cadence surfaces the quiet gaps before they ship

> [!reply] @vamsi_software Â· 2026-01-13T03:54:10+00:00
> @jarrodwatts I would love to do something like this.
> 
> But we have code rabbit on PRs which gives itâ€™s own reviews ðŸ’€

> [!reply] @olegpustovit Â· 2026-01-14T00:49:40+00:00
> @jarrodwatts I would also try adding an Codex on Claude Code hooks for specific actions, but that may add more latency to the overall execution of the agent

> [!reply] @terminalnotes Â· 2026-01-13T11:40:31+00:00
> @jarrodwatts Wouldn't a skill fit this problem better than an MCP?
> 
> Reason: doesn't polute the context window so much

> [!reply] @BrandGrowthOS Â· 2026-01-13T05:07:14+00:00
> @jarrodwatts Love this. Two-model loops are underrated: one â€œdoerâ€ + one â€œcriticâ€ catches the quiet mistakes before they ship. The real win is consistencyâ€”teams trust it when the second pass is predictable, not just smarter.

> [!reply] @askcodi Â· 2026-01-13T07:12:31+00:00
> @jarrodwatts my agents arguing about indentation might be the funniest thing in my repo

> [!reply] @joelreymont Â· 2026-01-13T04:56:10+00:00
> @jarrodwatts You donâ€™t need MCP for this. 
> 
> I do it with a skill and a loop in the prompt!

> [!reply] @0xmesuthere Â· 2026-01-13T03:11:28+00:00
> @jarrodwatts You should consider switching roles sometimes

> [!reply] @real_doved Â· 2026-01-13T17:48:43+00:00
> @jarrodwatts Is there an advantage to use codex as the supervisor compared to another session of Claude code ?

> [!reply] @seorce_ Â· 2026-01-13T17:20:42+00:00
> @jarrodwatts review loop catches edge

> [!reply] @socialmonkeyai Â· 2026-01-13T10:58:25+00:00
> @jarrodwatts That's a smart iterative approach. I find that process builds in a valuable layer of objectivity, especially with complex projects. I've been experimenting with a similar setup for my own workflow.

> [!reply] @Rajesh23MD Â· 2026-01-13T06:48:29+00:00
> @jarrodwatts All LLMâ€™s you can prompt to spin two agents to review, discuss, counter argue and provide / implement.

> [!reply] @joelreymont Â· 2026-01-13T04:57:51+00:00
> @jarrodwatts https://t.co/w8UFesSK4U

> [!reply] @YakutKhon Â· 2026-01-13T03:30:21+00:00
> @jarrodwatts I use the same workflow, but either my workflow is wrong or my Claude is setup pretty well, I rarely find any big issues from Codex, most are actually false positives.
> 
> Using Claude to re-check it's work though, works quite well.

> [!reply] @jonnyzzz Â· 2026-01-13T06:16:30+00:00
> @jarrodwatts I am doing the same, just explained ClaudeCode how to run the Codex CLI.

> [!reply] @dylkil Â· 2026-01-13T12:39:01+00:00
> @jarrodwatts Why use mcp vs a codex skill that tells claude how to call codex cli properly?

> [!reply] @Manitcor Â· 2026-01-13T06:00:05+00:00
> @jarrodwatts There was a paper on this a few months back, cross model eval is more effective than evals using the same models, most effective was using different vendors models against each other.

> [!reply] @shed_deli Â· 2026-01-13T06:34:34+00:00
> @jarrodwatts Logical.


> [!metrics]- Engagement & Metadata
> **Likes:** 866 Â· **Replies:** 60 Â· **Reposts:** 50 Â· **Views:** 62,946
> **Engagement Score:** 2,960
>
> **Source:** tips Â· **Quality:** â€”/10
> **Curated:** âœ— Â· **Reply:** âœ—
> **ID:** [2010905663753617560](https://x.com/jarrodwatts/status/2010905663753617560)