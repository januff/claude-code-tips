# Project Context for Tip Analysis

## What we actively use (DO NOT flag as new)
- Delegation pattern: Claude.ai for planning, Claude Code for execution
- Handoff documents for cross-instance communication
- MCP servers (GitHub, filesystem) for tool access
- SQLite as source of truth, Obsidian as read-only export
- Quality-filtered vault export with semantic filenames
- Skills (slash commands) with YAML frontmatter and progressive disclosure
- File-based planning (task_plan.md, STATUS.json)
- Pre-compact and session-end hooks for automatic wrap-up
- Compounding summaries (daily, weekly, goals-audit)

## What we're actively building
- Autonomous daily bookmark monitor (pipeline complete, analysis engine in progress)
- LLM-based tip classification for morning briefings
- Cross-project coordination with hall-of-fake sibling repo

## What we're experimenting with
- Cross-model review (Claude + Codex/Gemini for code critique)
- Obsidian CLI integration (lightweight â€” vault health checks only)
- Ralph Wiggum for long-running task recovery

## What we've decided to skip
- Beads/Agent Mail (no success stories beyond creator, $550/mo)
- Agent SDK (different use case than archive work)
- Cowork (underwhelming for dev-heavy workflows)
- Voice/STT loops (not our workflow)
- Clawdbot overnight builds (cool demo, wrong fit)

## What matters for classification
- Tips about techniques we already use are NOTED (unless they reveal something new)
- Tips about techniques we're building/experimenting with are EXPERIMENT
- Tips that are genuinely new AND high-signal AND directly applicable are ACT_NOW
- ACT_NOW should be rare (1-3 per batch, not 34)
- Author reputation matters: Boris Cherny, Anthropic staff, high-engagement creators carry more weight
- Engagement is context, not a threshold: 500 likes on a niche technique > 5000 likes on a generic take
